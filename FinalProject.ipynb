{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeDHRxa0um7l"
      },
      "outputs": [],
      "source": [
        "# [1. 환경 정의]\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class CardFlipGame:\n",
        "    def __init__(self, cards):\n",
        "        \"\"\"\n",
        "        cards: List of cards, where positive integers are point cards and 'X' is the penalty card.\n",
        "        \"\"\"\n",
        "        self.cards = cards\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        random.shuffle(self.cards)\n",
        "        self.remaining_cards = self.cards[:]\n",
        "        self.flipped_cards = []  # 뒤집은 카드 기록\n",
        "        self.score = 0\n",
        "        self.done = False\n",
        "        self.last_score_before_x = None  # X 직전 점수 기록\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        # 상태는 점수, 남은 카드 수, 남은 'X' 카드 수로 구성\n",
        "        return (self.score, len(self.remaining_cards), self.remaining_cards.count('X'))\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.done:\n",
        "            raise ValueError(\"Game is already finished.\")\n",
        "\n",
        "        if action == 0:  # Stop playing\n",
        "            self.done = True  # 게임 종료\n",
        "            return self.get_state(), self.score, self.done\n",
        "\n",
        "        if action == 1:  # Flip a card\n",
        "            flipped_card = self.remaining_cards.pop(0)  # 카드 제거\n",
        "            self.flipped_cards.append(flipped_card)  # 뒤집은 카드 기록\n",
        "            if flipped_card == 'X':\n",
        "                self.last_score_before_x = self.score  # X 이전 점수 저장\n",
        "                self.score = 0  # 점수 초기화\n",
        "                self.done = True  # 게임 종료\n",
        "            else:\n",
        "                self.score += flipped_card  # 점수 누적\n",
        "\n",
        "        return self.get_state(), self.score, self.done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-DeZb75uwOx"
      },
      "outputs": [],
      "source": [
        "# [2. 강화학습 알고리즘]\n",
        "\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95  # 할인율\n",
        "        self.epsilon = 1.0  # 탐험 비율\n",
        "        self.epsilon_min = 0.01  # 최소 탐험 비율\n",
        "        self.epsilon_decay = 0.995  # 탐험 비율 감소 속도\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()  # 타깃 네트워크\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
        "            tf.keras.layers.Dense(24, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
        "                      loss='mse')\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(np.array([state]))\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target += self.gamma * np.amax(self.target_model.predict(np.array([next_state]))[0])\n",
        "            target_f = self.model.predict(np.array([state]))\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(np.array([state]), target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())  # 타깃 네트워크 동기화"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [3. 훈련 루프]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 환경과 에이전트 초기화\n",
        "cards = []  # 카드 세트\n",
        "env = CardFlipGame(cards)\n",
        "state_size = 3  # (현재 점수, 남은 카드 수, 남은 X 카드 수)\n",
        "action_size = 2  # (0: 그만두기, 1: 카드 뒤집기)\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "episodes = 1000  # 학습 에피소드 수\n",
        "batch_size = 32\n",
        "\n",
        "# 에피소드 결과 저장\n",
        "rewards = []\n",
        "heights = []\n",
        "colors = []  # 성공: 파란색, 실패: 빨간색\n",
        "\n",
        "# 훈련 루프\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    state = np.array(state)\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "        action = agent.act(state)  # 행동 선택\n",
        "        next_state, reward, done = env.step(action)  # 행동 수행\n",
        "        next_state = np.array(next_state)\n",
        "\n",
        "        # 보상 계산\n",
        "        if done and env.last_score_before_x is not None:  # X를 뒤집었을 때\n",
        "            height = env.last_score_before_x\n",
        "            reward = 0\n",
        "            colors.append('red')  # 실패는 빨간색\n",
        "        elif done:  # 그만두기\n",
        "            height = env.score\n",
        "            reward = env.score\n",
        "            colors.append('blue')  # 성공은 파란색\n",
        "\n",
        "        agent.remember(state, action, reward, next_state, done)  # 경험 저장\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            x_feedback = f\", Last score before 'X': {env.last_score_before_x}\" if env.last_score_before_x is not None else \"\"\n",
        "            print(f\"Episode {e+1}/{episodes}, Reward: {reward}{x_feedback}\")\n",
        "            print(f\"Flipped cards in this episode: {env.flipped_cards}\")\n",
        "            rewards.append(reward)\n",
        "            heights.append(height)\n",
        "            break\n",
        "\n",
        "    # 경험 리플레이\n",
        "    agent.replay(batch_size)\n",
        "\n",
        "    if e % 10 == 0:\n",
        "        agent.update_target_model()  # 타깃 네트워크 갱신\n",
        "\n",
        "# 그래프 출력\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.bar(range(len(heights)), heights, color=colors, alpha=0.6)\n",
        "plt.title(\"Reward per Episode\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.legend([\"Success (Blue)\", \"Failure (Red)\"], loc=\"upper left\")\n",
        "plt.show()\n",
        "\n",
        "# 마지막 100회차 분석\n",
        "last_100_rewards = rewards[-100:]\n",
        "success_count = len([r for r, c in zip(last_100_rewards, colors[-100:]) if c == 'blue'])  # 파란색 성공 횟수\n",
        "average_reward = np.mean(last_100_rewards)\n",
        "\n",
        "print(f\"Last 100 episodes - Success count: {success_count}, Average reward: {average_reward:.2f}\")"
      ],
      "metadata": {
        "id": "ER_nRC3Klp8o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}